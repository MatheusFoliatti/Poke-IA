import ollama
from typing import Optional
from app.core.config import settings


class LlamaChat:
    """Classe para interagir com o modelo Llama via Ollama"""

    def __init__(self):
        self.model = settings.OLLAMA_MODEL

        # No Windows, 'localhost' pode resolver para ::1 (IPv6) causando WinError 10049.
        # For√ßamos 127.0.0.1 (IPv4) para garantir compatibilidade.
        ollama_host = settings.OLLAMA_BASE_URL.replace("localhost", "127.0.0.1")
        self.client = ollama.Client(host=ollama_host)

        self.system_prompt = """Voc√™ √© um assistente especializado em Pok√©mon chamado Pok√©dexAI.
Voc√™ ajuda treinadores com informa√ß√µes sobre Pok√©mon de forma clara e objetiva.

IMPORTANTE:
- Seja CONCISO: respostas com 3-5 frases no m√°ximo
- Use emojis quando apropriado
- Foque nas informa√ß√µes mais relevantes
- Se tiver dados do Pok√©mon, mencione-os brevemente
- Para compara√ß√µes, destaque as principais diferen√ßas E RECOMENDE um deles com justificativa

Exemplo de boa resposta para single:
"Pikachu √© um Pok√©mon El√©trico ic√¥nico! ‚ö° Com 35 de HP e 55 de ataque, √© r√°pido mas fr√°gil. Perfeito para batalhas que exigem velocidade!"

Exemplo de boa resposta para compara√ß√£o:
"Charizard tem mais ataque (84 vs 83) e velocidade superior (100 vs 78). üî• Blastoise √© mais defensivo com 100 de defesa. ‚úÖ Recomendo Charizard se voc√™ busca agressividade e velocidade, ideal para atacantes r√°pidos!"
"""

    async def generate_response(
        self, user_message: str, context: Optional[str] = None
    ) -> str:
        """Gera uma resposta usando o modelo Llama"""

        try:
            print(f"ü§ñ [LLM] Usando modelo: {self.model}")

            is_comparison = context and (
                "compara√ß√£o" in context.lower() or "vs" in context.lower()
            )

            messages = [{"role": "system", "content": self.system_prompt}]

            if context:
                if is_comparison:
                    messages.append(
                        {
                            "role": "user",
                            "content": f"""Contexto:
{context}

Pergunta: {user_message}

Fa√ßa uma compara√ß√£o completa:
1. Destaque as principais diferen√ßas nas stats (2 frases)
2. RECOMENDE qual √© melhor e JUSTIFIQUE baseado nas stats (2-3 frases)
3. Use emojis e seja objetivo""",
                        }
                    )
                else:
                    messages.append(
                        {
                            "role": "user",
                            "content": f"Contexto:\n{context}\n\nPergunta: {user_message}",
                        }
                    )
            else:
                messages.append({"role": "user", "content": user_message})

            print(f"üì§ [LLM] Enviando mensagem para Ollama...")

            # Usa o client com host fixo em 127.0.0.1
            response = self.client.chat(model=self.model, messages=messages)

            bot_response = response["message"]["content"]
            print(f"üì• [LLM] Resposta recebida: {bot_response[:100]}...")

            return bot_response

        except Exception as e:
            print(f"‚ùå [LLM] Erro ao gerar resposta: {e}")
            raise

    def check_ollama_connection(self) -> bool:
        """Verifica se o Ollama est√° dispon√≠vel"""
        try:
            self.client.list()
            print("‚úÖ [LLM] Ollama est√° dispon√≠vel")
            return True
        except Exception as e:
            print(f"‚ùå [LLM] Ollama n√£o est√° dispon√≠vel: {e}")
            return False


# Inst√¢ncia global
llama_chat = LlamaChat()

# Verifica conex√£o ao importar
llama_chat.check_ollama_connection()
