import ollama
from typing import Optional
from app.core.config import settings


class LlamaChat:
    """Classe para interagir com o modelo Llama via Ollama"""
    
    def __init__(self):
        self.model = settings.OLLAMA_MODEL
        self.system_prompt = """Voc√™ √© um assistente especializado em Pok√©mon chamado Pok√©dexAI.
Voc√™ ajuda treinadores com informa√ß√µes sobre Pok√©mon de forma clara e objetiva.

IMPORTANTE:
- Seja CONCISO: respostas com 3-5 frases no m√°ximo
- Use emojis quando apropriado
- Foque nas informa√ß√µes mais relevantes
- Se tiver dados do Pok√©mon, mencione-os brevemente
- Para compara√ß√µes, destaque as principais diferen√ßas E RECOMENDE um deles com justificativa

Exemplo de boa resposta para single:
"Pikachu √© um Pok√©mon El√©trico ic√¥nico! ‚ö° Com 35 de HP e 55 de ataque, √© r√°pido mas fr√°gil. Perfeito para batalhas que exigem velocidade!"

Exemplo de boa resposta para compara√ß√£o:
"Charizard tem mais ataque (84 vs 83) e velocidade superior (100 vs 78). üî• Blastoise √© mais defensivo com 100 de defesa. ‚úÖ Recomendo Charizard se voc√™ busca agressividade e velocidade, ideal para atacantes r√°pidos!"
"""
    
    async def generate_response(
        self, 
        user_message: str, 
        context: Optional[str] = None
    ) -> str:
        """Gera uma resposta usando o modelo Llama"""
        
        try:
            print(f"ü§ñ [LLM] Usando modelo: {self.model}")
            
            # Verificar se √© uma compara√ß√£o
            is_comparison = context and ("compara√ß√£o" in context.lower() or "vs" in context.lower())
            
            # Construir mensagens
            messages = [
                {
                    'role': 'system',
                    'content': self.system_prompt
                }
            ]
            
            # Adicionar contexto se houver
            if context:
                if is_comparison:
                    messages.append({
                        'role': 'user',
                        'content': f"""Contexto:
{context}

Pergunta: {user_message}

Fa√ßa uma compara√ß√£o completa:
1. Destaque as principais diferen√ßas nas stats (2 frases)
2. RECOMENDE qual √© melhor e JUSTIFIQUE baseado nas stats (2-3 frases)
3. Use emojis e seja objetivo"""
                    })
                else:
                    messages.append({
                        'role': 'user',
                        'content': f"Contexto:\n{context}\n\nPergunta: {user_message}"
                    })
            else:
                messages.append({
                    'role': 'user',
                    'content': user_message
                })
            
            print(f"üì§ [LLM] Enviando mensagem para Ollama...")
            
            # Chamar o Ollama (sem passar base_url, usa localhost:11434 por padr√£o)
            response = ollama.chat(
                model=self.model,
                messages=messages
            )
            
            bot_response = response['message']['content']
            
            print(f"üì• [LLM] Resposta recebida: {bot_response[:100]}...")
            
            return bot_response
            
        except Exception as e:
            print(f"‚ùå [LLM] Erro ao gerar resposta: {e}")
            raise  # Re-raise para o chat_service tratar com fallback
    
    def check_ollama_connection(self) -> bool:
        """Verifica se o Ollama est√° dispon√≠vel"""
        try:
            ollama.list()
            print("‚úÖ [LLM] Ollama est√° dispon√≠vel")
            return True
        except Exception as e:
            print(f"‚ùå [LLM] Ollama n√£o est√° dispon√≠vel: {e}")
            return False


# Inst√¢ncia global
llama_chat = LlamaChat()

# Verifica conex√£o ao importar
llama_chat.check_ollama_connection()